{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dataset and select x and y data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=pd.read_csv('./data/driver_clean_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first select all the possible data that we might consider as input data. Then, we will try different networks with different subsets of this big X vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xall = dataset[['Year','RaceResults','GridPos','QualiRes','NumRaces','AvgFinalPos','StdDevFinalPos','MedianFinalPos','AvgStartPos','StdDevStartPos','MedianStartPos']]\n",
    "y = dataset['Championship Position']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split in train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xall_train, Xall_test, y_train, y_test = train_test_split(Xall, y, random_state=111,test_size=0.3,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AvgFinalPos</th>\n",
       "      <th>StdDevFinalPos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>8.500000</td>\n",
       "      <td>8.817596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>664</th>\n",
       "      <td>7.421053</td>\n",
       "      <td>5.696867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>757</th>\n",
       "      <td>12.250000</td>\n",
       "      <td>3.973349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573</th>\n",
       "      <td>14.947368</td>\n",
       "      <td>6.108438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>15.055556</td>\n",
       "      <td>5.512332</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     AvgFinalPos  StdDevFinalPos\n",
       "59      8.500000        8.817596\n",
       "664     7.421053        5.696867\n",
       "757    12.250000        3.973349\n",
       "573    14.947368        6.108438\n",
       "487    15.055556        5.512332"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xall_train[['AvgFinalPos','StdDevFinalPos']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['Championship Position'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since all the input and output values have similar size and there is no more than an order of magnitude of difference between minimum and maximum, there is no need to normalize the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "yTrain = torch.from_numpy(np.float32(y_train.values))\n",
    "yTest = torch.from_numpy(np.float32(y_test.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModelPerEpoch(model,optimModel,lossModel,xtrain,ytrain):\n",
    "    #Set the model in training mode\n",
    "    model.train()\n",
    "    #Set to zero the gradient\n",
    "    optimModel.zero_grad()\n",
    "    #Evaluate model on training data\n",
    "    output = model(xtrain)\n",
    "    #Compute the loss\n",
    "    loss_ep = lossModel(output[:,0],ytrain)\n",
    "    #Backward propagation\n",
    "    loss_ep.backward()\n",
    "    #Update the weights of the model\n",
    "    optimModel.step()\n",
    "    return loss_ep.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(start_ep,num_ep,model,optimModel,lossModel,xtrain,ytrain):\n",
    "    loss_train_arr = []\n",
    "    for epoch in range(start_ep,num_ep+1):\n",
    "        #Execute the training function\n",
    "        loss_epoch = trainModelPerEpoch(model,optimModel,lossModel,xtrain,ytrain)\n",
    "        loss_train_arr.append(loss_epoch) \n",
    "        #Print update.\n",
    "        print('Epoch {:.0f} out of {:.0f}, Training Loss: {:.3e}'.format(epoch+1,num_ep,loss_epoch))\n",
    "        \n",
    "    return np.array(loss_train_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validateModel(model,lossModel,xval,yval):\n",
    "    output = model(xval)\n",
    "    #Compute the loss\n",
    "    loss_val = lossModel(output[:,0],yval)\n",
    "    #Backward propagation\n",
    "    print('Validation Loss: {:.3e}'.format(loss_val.item()))\n",
    "    \n",
    "    return loss_val.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Validation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainValModel(start_ep,num_ep,model,optimModel,lossModel,xtrain,ytrain,xval,yval):\n",
    "    loss_train_val_arr = []\n",
    "    for epoch in range(start_ep,num_ep+1):\n",
    "        #Execute the training function\n",
    "        loss_train_epoch = trainModelPerEpoch(model,optimModel,lossModel,xtrain,ytrain)\n",
    "        loss_val_epoch = validateModel(model,lossModel,xval,yval)\n",
    "        loss_train_val_arr.append([epoch+1,loss_train_epoch,loss_val_epoch]) \n",
    "        #Print update.\n",
    "        print('####Epoch {:.0f} out of {:.0f}###\\n\\t---Training Loss: {:.3e},\\n\\t---Validation Loss: {:.3e}'.format(epoch+1,num_ep,loss_train_epoch,loss_val_epoch))\n",
    "        \n",
    "    return np.array(loss_train_val_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_MLF1 = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I will construct different models with different inputs to compare their predictive power.\n",
    "They will all use dense layers. I will also test different hyperparameter configurations.\n",
    "The output  will not be hardcoded, i.e. the last layer will give a number between 1 and 46, which is the maximum Championship position found in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: Final position average and standard deviation as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmod1_train=torch.from_numpy(np.float32(Xall_train[['AvgFinalPos','StdDevFinalPos']].values))\n",
    "xmod1_test=torch.from_numpy(np.float32(Xall_test[['AvgFinalPos','StdDevFinalPos']].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Pytorch, this is done by defining a new class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLF1mod1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lay1 = nn.Linear(2,10)\n",
    "        self.lay2 = nn.Linear(10,20)\n",
    "        self.lay3 = nn.Linear(20,10)\n",
    "        self.lay4  = nn.Linear(10,1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.lay1(x))\n",
    "        x = F.relu(self.lay2(x))\n",
    "        x = F.relu(self.lay3(x))\n",
    "        x = self.lay4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = MLF1mod1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimMod1 = optim.Adam(model1.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=model1(xmod1_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 3.039e+02\n",
      "####Epoch 1 out of 350###\n",
      "\t---Training Loss: 2.792e+02,\n",
      "\t---Validation Loss: 3.039e+02\n",
      "Validation Loss: 3.036e+02\n",
      "####Epoch 2 out of 350###\n",
      "\t---Training Loss: 2.789e+02,\n",
      "\t---Validation Loss: 3.036e+02\n",
      "Validation Loss: 3.033e+02\n",
      "####Epoch 3 out of 350###\n",
      "\t---Training Loss: 2.786e+02,\n",
      "\t---Validation Loss: 3.033e+02\n",
      "Validation Loss: 3.030e+02\n",
      "####Epoch 4 out of 350###\n",
      "\t---Training Loss: 2.784e+02,\n",
      "\t---Validation Loss: 3.030e+02\n",
      "Validation Loss: 3.027e+02\n",
      "####Epoch 5 out of 350###\n",
      "\t---Training Loss: 2.781e+02,\n",
      "\t---Validation Loss: 3.027e+02\n",
      "Validation Loss: 3.024e+02\n",
      "####Epoch 6 out of 350###\n",
      "\t---Training Loss: 2.778e+02,\n",
      "\t---Validation Loss: 3.024e+02\n",
      "Validation Loss: 3.021e+02\n",
      "####Epoch 7 out of 350###\n",
      "\t---Training Loss: 2.776e+02,\n",
      "\t---Validation Loss: 3.021e+02\n",
      "Validation Loss: 3.018e+02\n",
      "####Epoch 8 out of 350###\n",
      "\t---Training Loss: 2.773e+02,\n",
      "\t---Validation Loss: 3.018e+02\n",
      "Validation Loss: 3.016e+02\n",
      "####Epoch 9 out of 350###\n",
      "\t---Training Loss: 2.771e+02,\n",
      "\t---Validation Loss: 3.016e+02\n",
      "Validation Loss: 3.013e+02\n",
      "####Epoch 10 out of 350###\n",
      "\t---Training Loss: 2.768e+02,\n",
      "\t---Validation Loss: 3.013e+02\n",
      "Validation Loss: 3.010e+02\n",
      "####Epoch 11 out of 350###\n",
      "\t---Training Loss: 2.765e+02,\n",
      "\t---Validation Loss: 3.010e+02\n",
      "Validation Loss: 3.007e+02\n",
      "####Epoch 12 out of 350###\n",
      "\t---Training Loss: 2.763e+02,\n",
      "\t---Validation Loss: 3.007e+02\n",
      "Validation Loss: 3.004e+02\n",
      "####Epoch 13 out of 350###\n",
      "\t---Training Loss: 2.760e+02,\n",
      "\t---Validation Loss: 3.004e+02\n",
      "Validation Loss: 3.001e+02\n",
      "####Epoch 14 out of 350###\n",
      "\t---Training Loss: 2.757e+02,\n",
      "\t---Validation Loss: 3.001e+02\n",
      "Validation Loss: 2.998e+02\n",
      "####Epoch 15 out of 350###\n",
      "\t---Training Loss: 2.755e+02,\n",
      "\t---Validation Loss: 2.998e+02\n",
      "Validation Loss: 2.995e+02\n",
      "####Epoch 16 out of 350###\n",
      "\t---Training Loss: 2.752e+02,\n",
      "\t---Validation Loss: 2.995e+02\n",
      "Validation Loss: 2.992e+02\n",
      "####Epoch 17 out of 350###\n",
      "\t---Training Loss: 2.749e+02,\n",
      "\t---Validation Loss: 2.992e+02\n",
      "Validation Loss: 2.988e+02\n",
      "####Epoch 18 out of 350###\n",
      "\t---Training Loss: 2.746e+02,\n",
      "\t---Validation Loss: 2.988e+02\n",
      "Validation Loss: 2.985e+02\n",
      "####Epoch 19 out of 350###\n",
      "\t---Training Loss: 2.743e+02,\n",
      "\t---Validation Loss: 2.985e+02\n",
      "Validation Loss: 2.982e+02\n",
      "####Epoch 20 out of 350###\n",
      "\t---Training Loss: 2.740e+02,\n",
      "\t---Validation Loss: 2.982e+02\n",
      "Validation Loss: 2.978e+02\n",
      "####Epoch 21 out of 350###\n",
      "\t---Training Loss: 2.737e+02,\n",
      "\t---Validation Loss: 2.978e+02\n",
      "Validation Loss: 2.975e+02\n",
      "####Epoch 22 out of 350###\n",
      "\t---Training Loss: 2.734e+02,\n",
      "\t---Validation Loss: 2.975e+02\n",
      "Validation Loss: 2.971e+02\n",
      "####Epoch 23 out of 350###\n",
      "\t---Training Loss: 2.731e+02,\n",
      "\t---Validation Loss: 2.971e+02\n",
      "Validation Loss: 2.968e+02\n",
      "####Epoch 24 out of 350###\n",
      "\t---Training Loss: 2.728e+02,\n",
      "\t---Validation Loss: 2.968e+02\n",
      "Validation Loss: 2.964e+02\n",
      "####Epoch 25 out of 350###\n",
      "\t---Training Loss: 2.725e+02,\n",
      "\t---Validation Loss: 2.964e+02\n",
      "Validation Loss: 2.961e+02\n",
      "####Epoch 26 out of 350###\n",
      "\t---Training Loss: 2.721e+02,\n",
      "\t---Validation Loss: 2.961e+02\n",
      "Validation Loss: 2.957e+02\n",
      "####Epoch 27 out of 350###\n",
      "\t---Training Loss: 2.718e+02,\n",
      "\t---Validation Loss: 2.957e+02\n",
      "Validation Loss: 2.953e+02\n",
      "####Epoch 28 out of 350###\n",
      "\t---Training Loss: 2.715e+02,\n",
      "\t---Validation Loss: 2.953e+02\n",
      "Validation Loss: 2.949e+02\n",
      "####Epoch 29 out of 350###\n",
      "\t---Training Loss: 2.711e+02,\n",
      "\t---Validation Loss: 2.949e+02\n",
      "Validation Loss: 2.945e+02\n",
      "####Epoch 30 out of 350###\n",
      "\t---Training Loss: 2.708e+02,\n",
      "\t---Validation Loss: 2.945e+02\n",
      "Validation Loss: 2.942e+02\n",
      "####Epoch 31 out of 350###\n",
      "\t---Training Loss: 2.704e+02,\n",
      "\t---Validation Loss: 2.942e+02\n",
      "Validation Loss: 2.938e+02\n",
      "####Epoch 32 out of 350###\n",
      "\t---Training Loss: 2.701e+02,\n",
      "\t---Validation Loss: 2.938e+02\n",
      "Validation Loss: 2.934e+02\n",
      "####Epoch 33 out of 350###\n",
      "\t---Training Loss: 2.697e+02,\n",
      "\t---Validation Loss: 2.934e+02\n",
      "Validation Loss: 2.930e+02\n",
      "####Epoch 34 out of 350###\n",
      "\t---Training Loss: 2.693e+02,\n",
      "\t---Validation Loss: 2.930e+02\n",
      "Validation Loss: 2.926e+02\n",
      "####Epoch 35 out of 350###\n",
      "\t---Training Loss: 2.690e+02,\n",
      "\t---Validation Loss: 2.926e+02\n",
      "Validation Loss: 2.922e+02\n",
      "####Epoch 36 out of 350###\n",
      "\t---Training Loss: 2.686e+02,\n",
      "\t---Validation Loss: 2.922e+02\n",
      "Validation Loss: 2.918e+02\n",
      "####Epoch 37 out of 350###\n",
      "\t---Training Loss: 2.682e+02,\n",
      "\t---Validation Loss: 2.918e+02\n",
      "Validation Loss: 2.914e+02\n",
      "####Epoch 38 out of 350###\n",
      "\t---Training Loss: 2.679e+02,\n",
      "\t---Validation Loss: 2.914e+02\n",
      "Validation Loss: 2.909e+02\n",
      "####Epoch 39 out of 350###\n",
      "\t---Training Loss: 2.675e+02,\n",
      "\t---Validation Loss: 2.909e+02\n",
      "Validation Loss: 2.905e+02\n",
      "####Epoch 40 out of 350###\n",
      "\t---Training Loss: 2.671e+02,\n",
      "\t---Validation Loss: 2.905e+02\n",
      "Validation Loss: 2.900e+02\n",
      "####Epoch 41 out of 350###\n",
      "\t---Training Loss: 2.667e+02,\n",
      "\t---Validation Loss: 2.900e+02\n",
      "Validation Loss: 2.896e+02\n",
      "####Epoch 42 out of 350###\n",
      "\t---Training Loss: 2.662e+02,\n",
      "\t---Validation Loss: 2.896e+02\n",
      "Validation Loss: 2.891e+02\n",
      "####Epoch 43 out of 350###\n",
      "\t---Training Loss: 2.658e+02,\n",
      "\t---Validation Loss: 2.891e+02\n",
      "Validation Loss: 2.886e+02\n",
      "####Epoch 44 out of 350###\n",
      "\t---Training Loss: 2.654e+02,\n",
      "\t---Validation Loss: 2.886e+02\n",
      "Validation Loss: 2.881e+02\n",
      "####Epoch 45 out of 350###\n",
      "\t---Training Loss: 2.649e+02,\n",
      "\t---Validation Loss: 2.881e+02\n",
      "Validation Loss: 2.875e+02\n",
      "####Epoch 46 out of 350###\n",
      "\t---Training Loss: 2.644e+02,\n",
      "\t---Validation Loss: 2.875e+02\n",
      "Validation Loss: 2.870e+02\n",
      "####Epoch 47 out of 350###\n",
      "\t---Training Loss: 2.640e+02,\n",
      "\t---Validation Loss: 2.870e+02\n",
      "Validation Loss: 2.864e+02\n",
      "####Epoch 48 out of 350###\n",
      "\t---Training Loss: 2.635e+02,\n",
      "\t---Validation Loss: 2.864e+02\n",
      "Validation Loss: 2.858e+02\n",
      "####Epoch 49 out of 350###\n",
      "\t---Training Loss: 2.629e+02,\n",
      "\t---Validation Loss: 2.858e+02\n",
      "Validation Loss: 2.852e+02\n",
      "####Epoch 50 out of 350###\n",
      "\t---Training Loss: 2.624e+02,\n",
      "\t---Validation Loss: 2.852e+02\n",
      "Validation Loss: 2.846e+02\n",
      "####Epoch 51 out of 350###\n",
      "\t---Training Loss: 2.619e+02,\n",
      "\t---Validation Loss: 2.846e+02\n",
      "Validation Loss: 2.840e+02\n",
      "####Epoch 52 out of 350###\n",
      "\t---Training Loss: 2.613e+02,\n",
      "\t---Validation Loss: 2.840e+02\n",
      "Validation Loss: 2.833e+02\n",
      "####Epoch 53 out of 350###\n",
      "\t---Training Loss: 2.607e+02,\n",
      "\t---Validation Loss: 2.833e+02\n",
      "Validation Loss: 2.826e+02\n",
      "####Epoch 54 out of 350###\n",
      "\t---Training Loss: 2.601e+02,\n",
      "\t---Validation Loss: 2.826e+02\n",
      "Validation Loss: 2.819e+02\n",
      "####Epoch 55 out of 350###\n",
      "\t---Training Loss: 2.595e+02,\n",
      "\t---Validation Loss: 2.819e+02\n",
      "Validation Loss: 2.812e+02\n",
      "####Epoch 56 out of 350###\n",
      "\t---Training Loss: 2.588e+02,\n",
      "\t---Validation Loss: 2.812e+02\n",
      "Validation Loss: 2.804e+02\n",
      "####Epoch 57 out of 350###\n",
      "\t---Training Loss: 2.581e+02,\n",
      "\t---Validation Loss: 2.804e+02\n",
      "Validation Loss: 2.796e+02\n",
      "####Epoch 58 out of 350###\n",
      "\t---Training Loss: 2.574e+02,\n",
      "\t---Validation Loss: 2.796e+02\n",
      "Validation Loss: 2.787e+02\n",
      "####Epoch 59 out of 350###\n",
      "\t---Training Loss: 2.567e+02,\n",
      "\t---Validation Loss: 2.787e+02\n",
      "Validation Loss: 2.778e+02\n",
      "####Epoch 60 out of 350###\n",
      "\t---Training Loss: 2.559e+02,\n",
      "\t---Validation Loss: 2.778e+02\n",
      "Validation Loss: 2.769e+02\n",
      "####Epoch 61 out of 350###\n",
      "\t---Training Loss: 2.551e+02,\n",
      "\t---Validation Loss: 2.769e+02\n",
      "Validation Loss: 2.758e+02\n",
      "####Epoch 62 out of 350###\n",
      "\t---Training Loss: 2.542e+02,\n",
      "\t---Validation Loss: 2.758e+02\n",
      "Validation Loss: 2.747e+02\n",
      "####Epoch 63 out of 350###\n",
      "\t---Training Loss: 2.532e+02,\n",
      "\t---Validation Loss: 2.747e+02\n",
      "Validation Loss: 2.735e+02\n",
      "####Epoch 64 out of 350###\n",
      "\t---Training Loss: 2.521e+02,\n",
      "\t---Validation Loss: 2.735e+02\n",
      "Validation Loss: 2.722e+02\n",
      "####Epoch 65 out of 350###\n",
      "\t---Training Loss: 2.510e+02,\n",
      "\t---Validation Loss: 2.722e+02\n",
      "Validation Loss: 2.709e+02\n",
      "####Epoch 66 out of 350###\n",
      "\t---Training Loss: 2.498e+02,\n",
      "\t---Validation Loss: 2.709e+02\n",
      "Validation Loss: 2.695e+02\n",
      "####Epoch 67 out of 350###\n",
      "\t---Training Loss: 2.486e+02,\n",
      "\t---Validation Loss: 2.695e+02\n",
      "Validation Loss: 2.680e+02\n",
      "####Epoch 68 out of 350###\n",
      "\t---Training Loss: 2.473e+02,\n",
      "\t---Validation Loss: 2.680e+02\n",
      "Validation Loss: 2.665e+02\n",
      "####Epoch 69 out of 350###\n",
      "\t---Training Loss: 2.459e+02,\n",
      "\t---Validation Loss: 2.665e+02\n",
      "Validation Loss: 2.649e+02\n",
      "####Epoch 70 out of 350###\n",
      "\t---Training Loss: 2.445e+02,\n",
      "\t---Validation Loss: 2.649e+02\n",
      "Validation Loss: 2.633e+02\n",
      "####Epoch 71 out of 350###\n",
      "\t---Training Loss: 2.431e+02,\n",
      "\t---Validation Loss: 2.633e+02\n",
      "Validation Loss: 2.616e+02\n",
      "####Epoch 72 out of 350###\n",
      "\t---Training Loss: 2.416e+02,\n",
      "\t---Validation Loss: 2.616e+02\n",
      "Validation Loss: 2.598e+02\n",
      "####Epoch 73 out of 350###\n",
      "\t---Training Loss: 2.400e+02,\n",
      "\t---Validation Loss: 2.598e+02\n",
      "Validation Loss: 2.581e+02\n",
      "####Epoch 74 out of 350###\n",
      "\t---Training Loss: 2.385e+02,\n",
      "\t---Validation Loss: 2.581e+02\n",
      "Validation Loss: 2.562e+02\n",
      "####Epoch 75 out of 350###\n",
      "\t---Training Loss: 2.368e+02,\n",
      "\t---Validation Loss: 2.562e+02\n",
      "Validation Loss: 2.543e+02\n",
      "####Epoch 76 out of 350###\n",
      "\t---Training Loss: 2.351e+02,\n",
      "\t---Validation Loss: 2.543e+02\n",
      "Validation Loss: 2.523e+02\n",
      "####Epoch 77 out of 350###\n",
      "\t---Training Loss: 2.334e+02,\n",
      "\t---Validation Loss: 2.523e+02\n",
      "Validation Loss: 2.503e+02\n",
      "####Epoch 78 out of 350###\n",
      "\t---Training Loss: 2.316e+02,\n",
      "\t---Validation Loss: 2.503e+02\n",
      "Validation Loss: 2.483e+02\n",
      "####Epoch 79 out of 350###\n",
      "\t---Training Loss: 2.297e+02,\n",
      "\t---Validation Loss: 2.483e+02\n",
      "Validation Loss: 2.461e+02\n",
      "####Epoch 80 out of 350###\n",
      "\t---Training Loss: 2.279e+02,\n",
      "\t---Validation Loss: 2.461e+02\n",
      "Validation Loss: 2.440e+02\n",
      "####Epoch 81 out of 350###\n",
      "\t---Training Loss: 2.259e+02,\n",
      "\t---Validation Loss: 2.440e+02\n",
      "Validation Loss: 2.417e+02\n",
      "####Epoch 82 out of 350###\n",
      "\t---Training Loss: 2.239e+02,\n",
      "\t---Validation Loss: 2.417e+02\n",
      "Validation Loss: 2.394e+02\n",
      "####Epoch 83 out of 350###\n",
      "\t---Training Loss: 2.218e+02,\n",
      "\t---Validation Loss: 2.394e+02\n",
      "Validation Loss: 2.370e+02\n",
      "####Epoch 84 out of 350###\n",
      "\t---Training Loss: 2.197e+02,\n",
      "\t---Validation Loss: 2.370e+02\n",
      "Validation Loss: 2.346e+02\n",
      "####Epoch 85 out of 350###\n",
      "\t---Training Loss: 2.175e+02,\n",
      "\t---Validation Loss: 2.346e+02\n",
      "Validation Loss: 2.321e+02\n",
      "####Epoch 86 out of 350###\n",
      "\t---Training Loss: 2.153e+02,\n",
      "\t---Validation Loss: 2.321e+02\n",
      "Validation Loss: 2.295e+02\n",
      "####Epoch 87 out of 350###\n",
      "\t---Training Loss: 2.130e+02,\n",
      "\t---Validation Loss: 2.295e+02\n",
      "Validation Loss: 2.269e+02\n",
      "####Epoch 88 out of 350###\n",
      "\t---Training Loss: 2.106e+02,\n",
      "\t---Validation Loss: 2.269e+02\n",
      "Validation Loss: 2.242e+02\n",
      "####Epoch 89 out of 350###\n",
      "\t---Training Loss: 2.082e+02,\n",
      "\t---Validation Loss: 2.242e+02\n",
      "Validation Loss: 2.214e+02\n",
      "####Epoch 90 out of 350###\n",
      "\t---Training Loss: 2.057e+02,\n",
      "\t---Validation Loss: 2.214e+02\n",
      "Validation Loss: 2.186e+02\n",
      "####Epoch 91 out of 350###\n",
      "\t---Training Loss: 2.032e+02,\n",
      "\t---Validation Loss: 2.186e+02\n",
      "Validation Loss: 2.157e+02\n",
      "####Epoch 92 out of 350###\n",
      "\t---Training Loss: 2.005e+02,\n",
      "\t---Validation Loss: 2.157e+02\n",
      "Validation Loss: 2.127e+02\n",
      "####Epoch 93 out of 350###\n",
      "\t---Training Loss: 1.979e+02,\n",
      "\t---Validation Loss: 2.127e+02\n",
      "Validation Loss: 2.097e+02\n",
      "####Epoch 94 out of 350###\n",
      "\t---Training Loss: 1.951e+02,\n",
      "\t---Validation Loss: 2.097e+02\n",
      "Validation Loss: 2.065e+02\n",
      "####Epoch 95 out of 350###\n",
      "\t---Training Loss: 1.923e+02,\n",
      "\t---Validation Loss: 2.065e+02\n",
      "Validation Loss: 2.033e+02\n",
      "####Epoch 96 out of 350###\n",
      "\t---Training Loss: 1.894e+02,\n",
      "\t---Validation Loss: 2.033e+02\n",
      "Validation Loss: 2.001e+02\n",
      "####Epoch 97 out of 350###\n",
      "\t---Training Loss: 1.864e+02,\n",
      "\t---Validation Loss: 2.001e+02\n",
      "Validation Loss: 1.967e+02\n",
      "####Epoch 98 out of 350###\n",
      "\t---Training Loss: 1.833e+02,\n",
      "\t---Validation Loss: 1.967e+02\n",
      "Validation Loss: 1.933e+02\n",
      "####Epoch 99 out of 350###\n",
      "\t---Training Loss: 1.802e+02,\n",
      "\t---Validation Loss: 1.933e+02\n",
      "Validation Loss: 1.897e+02\n",
      "####Epoch 100 out of 350###\n",
      "\t---Training Loss: 1.770e+02,\n",
      "\t---Validation Loss: 1.897e+02\n",
      "Validation Loss: 1.861e+02\n",
      "####Epoch 101 out of 350###\n",
      "\t---Training Loss: 1.737e+02,\n",
      "\t---Validation Loss: 1.861e+02\n",
      "Validation Loss: 1.824e+02\n",
      "####Epoch 102 out of 350###\n",
      "\t---Training Loss: 1.704e+02,\n",
      "\t---Validation Loss: 1.824e+02\n",
      "Validation Loss: 1.786e+02\n",
      "####Epoch 103 out of 350###\n",
      "\t---Training Loss: 1.669e+02,\n",
      "\t---Validation Loss: 1.786e+02\n",
      "Validation Loss: 1.746e+02\n",
      "####Epoch 104 out of 350###\n",
      "\t---Training Loss: 1.633e+02,\n",
      "\t---Validation Loss: 1.746e+02\n",
      "Validation Loss: 1.707e+02\n",
      "####Epoch 105 out of 350###\n",
      "\t---Training Loss: 1.597e+02,\n",
      "\t---Validation Loss: 1.707e+02\n",
      "Validation Loss: 1.666e+02\n",
      "####Epoch 106 out of 350###\n",
      "\t---Training Loss: 1.560e+02,\n",
      "\t---Validation Loss: 1.666e+02\n",
      "Validation Loss: 1.625e+02\n",
      "####Epoch 107 out of 350###\n",
      "\t---Training Loss: 1.522e+02,\n",
      "\t---Validation Loss: 1.625e+02\n",
      "Validation Loss: 1.583e+02\n",
      "####Epoch 108 out of 350###\n",
      "\t---Training Loss: 1.484e+02,\n",
      "\t---Validation Loss: 1.583e+02\n",
      "Validation Loss: 1.540e+02\n",
      "####Epoch 109 out of 350###\n",
      "\t---Training Loss: 1.445e+02,\n",
      "\t---Validation Loss: 1.540e+02\n",
      "Validation Loss: 1.498e+02\n",
      "####Epoch 110 out of 350###\n",
      "\t---Training Loss: 1.406e+02,\n",
      "\t---Validation Loss: 1.498e+02\n",
      "Validation Loss: 1.454e+02\n",
      "####Epoch 111 out of 350###\n",
      "\t---Training Loss: 1.366e+02,\n",
      "\t---Validation Loss: 1.454e+02\n",
      "Validation Loss: 1.411e+02\n",
      "####Epoch 112 out of 350###\n",
      "\t---Training Loss: 1.326e+02,\n",
      "\t---Validation Loss: 1.411e+02\n",
      "Validation Loss: 1.368e+02\n",
      "####Epoch 113 out of 350###\n",
      "\t---Training Loss: 1.286e+02,\n",
      "\t---Validation Loss: 1.368e+02\n",
      "Validation Loss: 1.324e+02\n",
      "####Epoch 114 out of 350###\n",
      "\t---Training Loss: 1.246e+02,\n",
      "\t---Validation Loss: 1.324e+02\n",
      "Validation Loss: 1.280e+02\n",
      "####Epoch 115 out of 350###\n",
      "\t---Training Loss: 1.206e+02,\n",
      "\t---Validation Loss: 1.280e+02\n",
      "Validation Loss: 1.236e+02\n",
      "####Epoch 116 out of 350###\n",
      "\t---Training Loss: 1.165e+02,\n",
      "\t---Validation Loss: 1.236e+02\n",
      "Validation Loss: 1.193e+02\n",
      "####Epoch 117 out of 350###\n",
      "\t---Training Loss: 1.125e+02,\n",
      "\t---Validation Loss: 1.193e+02\n",
      "Validation Loss: 1.149e+02\n",
      "####Epoch 118 out of 350###\n",
      "\t---Training Loss: 1.085e+02,\n",
      "\t---Validation Loss: 1.149e+02\n",
      "Validation Loss: 1.107e+02\n",
      "####Epoch 119 out of 350###\n",
      "\t---Training Loss: 1.046e+02,\n",
      "\t---Validation Loss: 1.107e+02\n",
      "Validation Loss: 1.064e+02\n",
      "####Epoch 120 out of 350###\n",
      "\t---Training Loss: 1.007e+02,\n",
      "\t---Validation Loss: 1.064e+02\n",
      "Validation Loss: 1.023e+02\n",
      "####Epoch 121 out of 350###\n",
      "\t---Training Loss: 9.679e+01,\n",
      "\t---Validation Loss: 1.023e+02\n",
      "Validation Loss: 9.817e+01\n",
      "####Epoch 122 out of 350###\n",
      "\t---Training Loss: 9.298e+01,\n",
      "\t---Validation Loss: 9.817e+01\n",
      "Validation Loss: 9.415e+01\n",
      "####Epoch 123 out of 350###\n",
      "\t---Training Loss: 8.924e+01,\n",
      "\t---Validation Loss: 9.415e+01\n",
      "Validation Loss: 9.021e+01\n",
      "####Epoch 124 out of 350###\n",
      "\t---Training Loss: 8.557e+01,\n",
      "\t---Validation Loss: 9.021e+01\n",
      "Validation Loss: 8.638e+01\n",
      "####Epoch 125 out of 350###\n",
      "\t---Training Loss: 8.200e+01,\n",
      "\t---Validation Loss: 8.638e+01\n",
      "Validation Loss: 8.265e+01\n",
      "####Epoch 126 out of 350###\n",
      "\t---Training Loss: 7.852e+01,\n",
      "\t---Validation Loss: 8.265e+01\n",
      "Validation Loss: 7.904e+01\n",
      "####Epoch 127 out of 350###\n",
      "\t---Training Loss: 7.515e+01,\n",
      "\t---Validation Loss: 7.904e+01\n",
      "Validation Loss: 7.556e+01\n",
      "####Epoch 128 out of 350###\n",
      "\t---Training Loss: 7.189e+01,\n",
      "\t---Validation Loss: 7.556e+01\n",
      "Validation Loss: 7.221e+01\n",
      "####Epoch 129 out of 350###\n",
      "\t---Training Loss: 6.876e+01,\n",
      "\t---Validation Loss: 7.221e+01\n",
      "Validation Loss: 6.901e+01\n",
      "####Epoch 130 out of 350###\n",
      "\t---Training Loss: 6.575e+01,\n",
      "\t---Validation Loss: 6.901e+01\n",
      "Validation Loss: 6.595e+01\n",
      "####Epoch 131 out of 350###\n",
      "\t---Training Loss: 6.288e+01,\n",
      "\t---Validation Loss: 6.595e+01\n",
      "Validation Loss: 6.305e+01\n",
      "####Epoch 132 out of 350###\n",
      "\t---Training Loss: 6.015e+01,\n",
      "\t---Validation Loss: 6.305e+01\n",
      "Validation Loss: 6.031e+01\n",
      "####Epoch 133 out of 350###\n",
      "\t---Training Loss: 5.757e+01,\n",
      "\t---Validation Loss: 6.031e+01\n",
      "Validation Loss: 5.774e+01\n",
      "####Epoch 134 out of 350###\n",
      "\t---Training Loss: 5.515e+01,\n",
      "\t---Validation Loss: 5.774e+01\n",
      "Validation Loss: 5.534e+01\n",
      "####Epoch 135 out of 350###\n",
      "\t---Training Loss: 5.289e+01,\n",
      "\t---Validation Loss: 5.534e+01\n",
      "Validation Loss: 5.311e+01\n",
      "####Epoch 136 out of 350###\n",
      "\t---Training Loss: 5.079e+01,\n",
      "\t---Validation Loss: 5.311e+01\n",
      "Validation Loss: 5.106e+01\n",
      "####Epoch 137 out of 350###\n",
      "\t---Training Loss: 4.886e+01,\n",
      "\t---Validation Loss: 5.106e+01\n",
      "Validation Loss: 4.917e+01\n",
      "####Epoch 138 out of 350###\n",
      "\t---Training Loss: 4.709e+01,\n",
      "\t---Validation Loss: 4.917e+01\n",
      "Validation Loss: 4.746e+01\n",
      "####Epoch 139 out of 350###\n",
      "\t---Training Loss: 4.548e+01,\n",
      "\t---Validation Loss: 4.746e+01\n",
      "Validation Loss: 4.591e+01\n",
      "####Epoch 140 out of 350###\n",
      "\t---Training Loss: 4.402e+01,\n",
      "\t---Validation Loss: 4.591e+01\n",
      "Validation Loss: 4.452e+01\n",
      "####Epoch 141 out of 350###\n",
      "\t---Training Loss: 4.272e+01,\n",
      "\t---Validation Loss: 4.452e+01\n",
      "Validation Loss: 4.328e+01\n",
      "####Epoch 142 out of 350###\n",
      "\t---Training Loss: 4.158e+01,\n",
      "\t---Validation Loss: 4.328e+01\n",
      "Validation Loss: 4.219e+01\n",
      "####Epoch 143 out of 350###\n",
      "\t---Training Loss: 4.057e+01,\n",
      "\t---Validation Loss: 4.219e+01\n",
      "Validation Loss: 4.123e+01\n",
      "####Epoch 144 out of 350###\n",
      "\t---Training Loss: 3.969e+01,\n",
      "\t---Validation Loss: 4.123e+01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 4.040e+01\n",
      "####Epoch 145 out of 350###\n",
      "\t---Training Loss: 3.894e+01,\n",
      "\t---Validation Loss: 4.040e+01\n",
      "Validation Loss: 3.968e+01\n",
      "####Epoch 146 out of 350###\n",
      "\t---Training Loss: 3.830e+01,\n",
      "\t---Validation Loss: 3.968e+01\n",
      "Validation Loss: 3.906e+01\n",
      "####Epoch 147 out of 350###\n",
      "\t---Training Loss: 3.776e+01,\n",
      "\t---Validation Loss: 3.906e+01\n",
      "Validation Loss: 3.854e+01\n",
      "####Epoch 148 out of 350###\n",
      "\t---Training Loss: 3.731e+01,\n",
      "\t---Validation Loss: 3.854e+01\n",
      "Validation Loss: 3.809e+01\n",
      "####Epoch 149 out of 350###\n",
      "\t---Training Loss: 3.693e+01,\n",
      "\t---Validation Loss: 3.809e+01\n",
      "Validation Loss: 3.770e+01\n",
      "####Epoch 150 out of 350###\n",
      "\t---Training Loss: 3.662e+01,\n",
      "\t---Validation Loss: 3.770e+01\n",
      "Validation Loss: 3.737e+01\n",
      "####Epoch 151 out of 350###\n",
      "\t---Training Loss: 3.635e+01,\n",
      "\t---Validation Loss: 3.737e+01\n",
      "Validation Loss: 3.708e+01\n",
      "####Epoch 152 out of 350###\n",
      "\t---Training Loss: 3.613e+01,\n",
      "\t---Validation Loss: 3.708e+01\n",
      "Validation Loss: 3.682e+01\n",
      "####Epoch 153 out of 350###\n",
      "\t---Training Loss: 3.593e+01,\n",
      "\t---Validation Loss: 3.682e+01\n",
      "Validation Loss: 3.659e+01\n",
      "####Epoch 154 out of 350###\n",
      "\t---Training Loss: 3.576e+01,\n",
      "\t---Validation Loss: 3.659e+01\n",
      "Validation Loss: 3.638e+01\n",
      "####Epoch 155 out of 350###\n",
      "\t---Training Loss: 3.559e+01,\n",
      "\t---Validation Loss: 3.638e+01\n",
      "Validation Loss: 3.617e+01\n",
      "####Epoch 156 out of 350###\n",
      "\t---Training Loss: 3.543e+01,\n",
      "\t---Validation Loss: 3.617e+01\n",
      "Validation Loss: 3.597e+01\n",
      "####Epoch 157 out of 350###\n",
      "\t---Training Loss: 3.527e+01,\n",
      "\t---Validation Loss: 3.597e+01\n",
      "Validation Loss: 3.578e+01\n",
      "####Epoch 158 out of 350###\n",
      "\t---Training Loss: 3.511e+01,\n",
      "\t---Validation Loss: 3.578e+01\n",
      "Validation Loss: 3.558e+01\n",
      "####Epoch 159 out of 350###\n",
      "\t---Training Loss: 3.493e+01,\n",
      "\t---Validation Loss: 3.558e+01\n",
      "Validation Loss: 3.537e+01\n",
      "####Epoch 160 out of 350###\n",
      "\t---Training Loss: 3.475e+01,\n",
      "\t---Validation Loss: 3.537e+01\n",
      "Validation Loss: 3.517e+01\n",
      "####Epoch 161 out of 350###\n",
      "\t---Training Loss: 3.455e+01,\n",
      "\t---Validation Loss: 3.517e+01\n",
      "Validation Loss: 3.496e+01\n",
      "####Epoch 162 out of 350###\n",
      "\t---Training Loss: 3.435e+01,\n",
      "\t---Validation Loss: 3.496e+01\n",
      "Validation Loss: 3.474e+01\n",
      "####Epoch 163 out of 350###\n",
      "\t---Training Loss: 3.413e+01,\n",
      "\t---Validation Loss: 3.474e+01\n",
      "Validation Loss: 3.451e+01\n",
      "####Epoch 164 out of 350###\n",
      "\t---Training Loss: 3.389e+01,\n",
      "\t---Validation Loss: 3.451e+01\n",
      "Validation Loss: 3.428e+01\n",
      "####Epoch 165 out of 350###\n",
      "\t---Training Loss: 3.365e+01,\n",
      "\t---Validation Loss: 3.428e+01\n",
      "Validation Loss: 3.405e+01\n",
      "####Epoch 166 out of 350###\n",
      "\t---Training Loss: 3.339e+01,\n",
      "\t---Validation Loss: 3.405e+01\n",
      "Validation Loss: 3.381e+01\n",
      "####Epoch 167 out of 350###\n",
      "\t---Training Loss: 3.313e+01,\n",
      "\t---Validation Loss: 3.381e+01\n",
      "Validation Loss: 3.356e+01\n",
      "####Epoch 168 out of 350###\n",
      "\t---Training Loss: 3.286e+01,\n",
      "\t---Validation Loss: 3.356e+01\n",
      "Validation Loss: 3.331e+01\n",
      "####Epoch 169 out of 350###\n",
      "\t---Training Loss: 3.258e+01,\n",
      "\t---Validation Loss: 3.331e+01\n",
      "Validation Loss: 3.306e+01\n",
      "####Epoch 170 out of 350###\n",
      "\t---Training Loss: 3.231e+01,\n",
      "\t---Validation Loss: 3.306e+01\n",
      "Validation Loss: 3.280e+01\n",
      "####Epoch 171 out of 350###\n",
      "\t---Training Loss: 3.203e+01,\n",
      "\t---Validation Loss: 3.280e+01\n",
      "Validation Loss: 3.254e+01\n",
      "####Epoch 172 out of 350###\n",
      "\t---Training Loss: 3.175e+01,\n",
      "\t---Validation Loss: 3.254e+01\n",
      "Validation Loss: 3.229e+01\n",
      "####Epoch 173 out of 350###\n",
      "\t---Training Loss: 3.147e+01,\n",
      "\t---Validation Loss: 3.229e+01\n",
      "Validation Loss: 3.203e+01\n",
      "####Epoch 174 out of 350###\n",
      "\t---Training Loss: 3.119e+01,\n",
      "\t---Validation Loss: 3.203e+01\n",
      "Validation Loss: 3.177e+01\n",
      "####Epoch 175 out of 350###\n",
      "\t---Training Loss: 3.091e+01,\n",
      "\t---Validation Loss: 3.177e+01\n",
      "Validation Loss: 3.152e+01\n",
      "####Epoch 176 out of 350###\n",
      "\t---Training Loss: 3.063e+01,\n",
      "\t---Validation Loss: 3.152e+01\n",
      "Validation Loss: 3.127e+01\n",
      "####Epoch 177 out of 350###\n",
      "\t---Training Loss: 3.035e+01,\n",
      "\t---Validation Loss: 3.127e+01\n",
      "Validation Loss: 3.101e+01\n",
      "####Epoch 178 out of 350###\n",
      "\t---Training Loss: 3.008e+01,\n",
      "\t---Validation Loss: 3.101e+01\n",
      "Validation Loss: 3.076e+01\n",
      "####Epoch 179 out of 350###\n",
      "\t---Training Loss: 2.981e+01,\n",
      "\t---Validation Loss: 3.076e+01\n",
      "Validation Loss: 3.051e+01\n",
      "####Epoch 180 out of 350###\n",
      "\t---Training Loss: 2.954e+01,\n",
      "\t---Validation Loss: 3.051e+01\n",
      "Validation Loss: 3.026e+01\n",
      "####Epoch 181 out of 350###\n",
      "\t---Training Loss: 2.927e+01,\n",
      "\t---Validation Loss: 3.026e+01\n",
      "Validation Loss: 3.001e+01\n",
      "####Epoch 182 out of 350###\n",
      "\t---Training Loss: 2.901e+01,\n",
      "\t---Validation Loss: 3.001e+01\n",
      "Validation Loss: 2.975e+01\n",
      "####Epoch 183 out of 350###\n",
      "\t---Training Loss: 2.875e+01,\n",
      "\t---Validation Loss: 2.975e+01\n",
      "Validation Loss: 2.951e+01\n",
      "####Epoch 184 out of 350###\n",
      "\t---Training Loss: 2.849e+01,\n",
      "\t---Validation Loss: 2.951e+01\n",
      "Validation Loss: 2.926e+01\n",
      "####Epoch 185 out of 350###\n",
      "\t---Training Loss: 2.823e+01,\n",
      "\t---Validation Loss: 2.926e+01\n",
      "Validation Loss: 2.902e+01\n",
      "####Epoch 186 out of 350###\n",
      "\t---Training Loss: 2.798e+01,\n",
      "\t---Validation Loss: 2.902e+01\n",
      "Validation Loss: 2.877e+01\n",
      "####Epoch 187 out of 350###\n",
      "\t---Training Loss: 2.773e+01,\n",
      "\t---Validation Loss: 2.877e+01\n",
      "Validation Loss: 2.853e+01\n",
      "####Epoch 188 out of 350###\n",
      "\t---Training Loss: 2.748e+01,\n",
      "\t---Validation Loss: 2.853e+01\n",
      "Validation Loss: 2.830e+01\n",
      "####Epoch 189 out of 350###\n",
      "\t---Training Loss: 2.723e+01,\n",
      "\t---Validation Loss: 2.830e+01\n",
      "Validation Loss: 2.806e+01\n",
      "####Epoch 190 out of 350###\n",
      "\t---Training Loss: 2.698e+01,\n",
      "\t---Validation Loss: 2.806e+01\n",
      "Validation Loss: 2.782e+01\n",
      "####Epoch 191 out of 350###\n",
      "\t---Training Loss: 2.674e+01,\n",
      "\t---Validation Loss: 2.782e+01\n",
      "Validation Loss: 2.758e+01\n",
      "####Epoch 192 out of 350###\n",
      "\t---Training Loss: 2.650e+01,\n",
      "\t---Validation Loss: 2.758e+01\n",
      "Validation Loss: 2.735e+01\n",
      "####Epoch 193 out of 350###\n",
      "\t---Training Loss: 2.627e+01,\n",
      "\t---Validation Loss: 2.735e+01\n",
      "Validation Loss: 2.711e+01\n",
      "####Epoch 194 out of 350###\n",
      "\t---Training Loss: 2.603e+01,\n",
      "\t---Validation Loss: 2.711e+01\n",
      "Validation Loss: 2.688e+01\n",
      "####Epoch 195 out of 350###\n",
      "\t---Training Loss: 2.580e+01,\n",
      "\t---Validation Loss: 2.688e+01\n",
      "Validation Loss: 2.664e+01\n",
      "####Epoch 196 out of 350###\n",
      "\t---Training Loss: 2.558e+01,\n",
      "\t---Validation Loss: 2.664e+01\n",
      "Validation Loss: 2.642e+01\n",
      "####Epoch 197 out of 350###\n",
      "\t---Training Loss: 2.536e+01,\n",
      "\t---Validation Loss: 2.642e+01\n",
      "Validation Loss: 2.620e+01\n",
      "####Epoch 198 out of 350###\n",
      "\t---Training Loss: 2.514e+01,\n",
      "\t---Validation Loss: 2.620e+01\n",
      "Validation Loss: 2.598e+01\n",
      "####Epoch 199 out of 350###\n",
      "\t---Training Loss: 2.492e+01,\n",
      "\t---Validation Loss: 2.598e+01\n",
      "Validation Loss: 2.576e+01\n",
      "####Epoch 200 out of 350###\n",
      "\t---Training Loss: 2.471e+01,\n",
      "\t---Validation Loss: 2.576e+01\n",
      "Validation Loss: 2.554e+01\n",
      "####Epoch 201 out of 350###\n",
      "\t---Training Loss: 2.449e+01,\n",
      "\t---Validation Loss: 2.554e+01\n",
      "Validation Loss: 2.533e+01\n",
      "####Epoch 202 out of 350###\n",
      "\t---Training Loss: 2.428e+01,\n",
      "\t---Validation Loss: 2.533e+01\n",
      "Validation Loss: 2.511e+01\n",
      "####Epoch 203 out of 350###\n",
      "\t---Training Loss: 2.407e+01,\n",
      "\t---Validation Loss: 2.511e+01\n",
      "Validation Loss: 2.489e+01\n",
      "####Epoch 204 out of 350###\n",
      "\t---Training Loss: 2.386e+01,\n",
      "\t---Validation Loss: 2.489e+01\n",
      "Validation Loss: 2.468e+01\n",
      "####Epoch 205 out of 350###\n",
      "\t---Training Loss: 2.365e+01,\n",
      "\t---Validation Loss: 2.468e+01\n",
      "Validation Loss: 2.447e+01\n",
      "####Epoch 206 out of 350###\n",
      "\t---Training Loss: 2.345e+01,\n",
      "\t---Validation Loss: 2.447e+01\n",
      "Validation Loss: 2.427e+01\n",
      "####Epoch 207 out of 350###\n",
      "\t---Training Loss: 2.325e+01,\n",
      "\t---Validation Loss: 2.427e+01\n",
      "Validation Loss: 2.407e+01\n",
      "####Epoch 208 out of 350###\n",
      "\t---Training Loss: 2.305e+01,\n",
      "\t---Validation Loss: 2.407e+01\n",
      "Validation Loss: 2.388e+01\n",
      "####Epoch 209 out of 350###\n",
      "\t---Training Loss: 2.286e+01,\n",
      "\t---Validation Loss: 2.388e+01\n",
      "Validation Loss: 2.369e+01\n",
      "####Epoch 210 out of 350###\n",
      "\t---Training Loss: 2.267e+01,\n",
      "\t---Validation Loss: 2.369e+01\n",
      "Validation Loss: 2.350e+01\n",
      "####Epoch 211 out of 350###\n",
      "\t---Training Loss: 2.248e+01,\n",
      "\t---Validation Loss: 2.350e+01\n",
      "Validation Loss: 2.332e+01\n",
      "####Epoch 212 out of 350###\n",
      "\t---Training Loss: 2.230e+01,\n",
      "\t---Validation Loss: 2.332e+01\n",
      "Validation Loss: 2.314e+01\n",
      "####Epoch 213 out of 350###\n",
      "\t---Training Loss: 2.211e+01,\n",
      "\t---Validation Loss: 2.314e+01\n",
      "Validation Loss: 2.296e+01\n",
      "####Epoch 214 out of 350###\n",
      "\t---Training Loss: 2.193e+01,\n",
      "\t---Validation Loss: 2.296e+01\n",
      "Validation Loss: 2.278e+01\n",
      "####Epoch 215 out of 350###\n",
      "\t---Training Loss: 2.174e+01,\n",
      "\t---Validation Loss: 2.278e+01\n",
      "Validation Loss: 2.261e+01\n",
      "####Epoch 216 out of 350###\n",
      "\t---Training Loss: 2.156e+01,\n",
      "\t---Validation Loss: 2.261e+01\n",
      "Validation Loss: 2.244e+01\n",
      "####Epoch 217 out of 350###\n",
      "\t---Training Loss: 2.138e+01,\n",
      "\t---Validation Loss: 2.244e+01\n",
      "Validation Loss: 2.227e+01\n",
      "####Epoch 218 out of 350###\n",
      "\t---Training Loss: 2.120e+01,\n",
      "\t---Validation Loss: 2.227e+01\n",
      "Validation Loss: 2.211e+01\n",
      "####Epoch 219 out of 350###\n",
      "\t---Training Loss: 2.103e+01,\n",
      "\t---Validation Loss: 2.211e+01\n",
      "Validation Loss: 2.195e+01\n",
      "####Epoch 220 out of 350###\n",
      "\t---Training Loss: 2.085e+01,\n",
      "\t---Validation Loss: 2.195e+01\n",
      "Validation Loss: 2.179e+01\n",
      "####Epoch 221 out of 350###\n",
      "\t---Training Loss: 2.068e+01,\n",
      "\t---Validation Loss: 2.179e+01\n",
      "Validation Loss: 2.164e+01\n",
      "####Epoch 222 out of 350###\n",
      "\t---Training Loss: 2.051e+01,\n",
      "\t---Validation Loss: 2.164e+01\n",
      "Validation Loss: 2.149e+01\n",
      "####Epoch 223 out of 350###\n",
      "\t---Training Loss: 2.034e+01,\n",
      "\t---Validation Loss: 2.149e+01\n",
      "Validation Loss: 2.134e+01\n",
      "####Epoch 224 out of 350###\n",
      "\t---Training Loss: 2.017e+01,\n",
      "\t---Validation Loss: 2.134e+01\n",
      "Validation Loss: 2.119e+01\n",
      "####Epoch 225 out of 350###\n",
      "\t---Training Loss: 2.001e+01,\n",
      "\t---Validation Loss: 2.119e+01\n",
      "Validation Loss: 2.105e+01\n",
      "####Epoch 226 out of 350###\n",
      "\t---Training Loss: 1.985e+01,\n",
      "\t---Validation Loss: 2.105e+01\n",
      "Validation Loss: 2.091e+01\n",
      "####Epoch 227 out of 350###\n",
      "\t---Training Loss: 1.969e+01,\n",
      "\t---Validation Loss: 2.091e+01\n",
      "Validation Loss: 2.077e+01\n",
      "####Epoch 228 out of 350###\n",
      "\t---Training Loss: 1.953e+01,\n",
      "\t---Validation Loss: 2.077e+01\n",
      "Validation Loss: 2.063e+01\n",
      "####Epoch 229 out of 350###\n",
      "\t---Training Loss: 1.937e+01,\n",
      "\t---Validation Loss: 2.063e+01\n",
      "Validation Loss: 2.049e+01\n",
      "####Epoch 230 out of 350###\n",
      "\t---Training Loss: 1.922e+01,\n",
      "\t---Validation Loss: 2.049e+01\n",
      "Validation Loss: 2.035e+01\n",
      "####Epoch 231 out of 350###\n",
      "\t---Training Loss: 1.907e+01,\n",
      "\t---Validation Loss: 2.035e+01\n",
      "Validation Loss: 2.022e+01\n",
      "####Epoch 232 out of 350###\n",
      "\t---Training Loss: 1.891e+01,\n",
      "\t---Validation Loss: 2.022e+01\n",
      "Validation Loss: 2.009e+01\n",
      "####Epoch 233 out of 350###\n",
      "\t---Training Loss: 1.876e+01,\n",
      "\t---Validation Loss: 2.009e+01\n",
      "Validation Loss: 1.996e+01\n",
      "####Epoch 234 out of 350###\n",
      "\t---Training Loss: 1.862e+01,\n",
      "\t---Validation Loss: 1.996e+01\n",
      "Validation Loss: 1.983e+01\n",
      "####Epoch 235 out of 350###\n",
      "\t---Training Loss: 1.847e+01,\n",
      "\t---Validation Loss: 1.983e+01\n",
      "Validation Loss: 1.970e+01\n",
      "####Epoch 236 out of 350###\n",
      "\t---Training Loss: 1.833e+01,\n",
      "\t---Validation Loss: 1.970e+01\n",
      "Validation Loss: 1.958e+01\n",
      "####Epoch 237 out of 350###\n",
      "\t---Training Loss: 1.819e+01,\n",
      "\t---Validation Loss: 1.958e+01\n",
      "Validation Loss: 1.946e+01\n",
      "####Epoch 238 out of 350###\n",
      "\t---Training Loss: 1.805e+01,\n",
      "\t---Validation Loss: 1.946e+01\n",
      "Validation Loss: 1.934e+01\n",
      "####Epoch 239 out of 350###\n",
      "\t---Training Loss: 1.791e+01,\n",
      "\t---Validation Loss: 1.934e+01\n",
      "Validation Loss: 1.923e+01\n",
      "####Epoch 240 out of 350###\n",
      "\t---Training Loss: 1.778e+01,\n",
      "\t---Validation Loss: 1.923e+01\n",
      "Validation Loss: 1.912e+01\n",
      "####Epoch 241 out of 350###\n",
      "\t---Training Loss: 1.765e+01,\n",
      "\t---Validation Loss: 1.912e+01\n",
      "Validation Loss: 1.901e+01\n",
      "####Epoch 242 out of 350###\n",
      "\t---Training Loss: 1.753e+01,\n",
      "\t---Validation Loss: 1.901e+01\n",
      "Validation Loss: 1.891e+01\n",
      "####Epoch 243 out of 350###\n",
      "\t---Training Loss: 1.740e+01,\n",
      "\t---Validation Loss: 1.891e+01\n",
      "Validation Loss: 1.881e+01\n",
      "####Epoch 244 out of 350###\n",
      "\t---Training Loss: 1.729e+01,\n",
      "\t---Validation Loss: 1.881e+01\n",
      "Validation Loss: 1.872e+01\n",
      "####Epoch 245 out of 350###\n",
      "\t---Training Loss: 1.717e+01,\n",
      "\t---Validation Loss: 1.872e+01\n",
      "Validation Loss: 1.863e+01\n",
      "####Epoch 246 out of 350###\n",
      "\t---Training Loss: 1.705e+01,\n",
      "\t---Validation Loss: 1.863e+01\n",
      "Validation Loss: 1.854e+01\n",
      "####Epoch 247 out of 350###\n",
      "\t---Training Loss: 1.694e+01,\n",
      "\t---Validation Loss: 1.854e+01\n",
      "Validation Loss: 1.846e+01\n",
      "####Epoch 248 out of 350###\n",
      "\t---Training Loss: 1.683e+01,\n",
      "\t---Validation Loss: 1.846e+01\n",
      "Validation Loss: 1.839e+01\n",
      "####Epoch 249 out of 350###\n",
      "\t---Training Loss: 1.673e+01,\n",
      "\t---Validation Loss: 1.839e+01\n",
      "Validation Loss: 1.831e+01\n",
      "####Epoch 250 out of 350###\n",
      "\t---Training Loss: 1.663e+01,\n",
      "\t---Validation Loss: 1.831e+01\n",
      "Validation Loss: 1.825e+01\n",
      "####Epoch 251 out of 350###\n",
      "\t---Training Loss: 1.653e+01,\n",
      "\t---Validation Loss: 1.825e+01\n",
      "Validation Loss: 1.818e+01\n",
      "####Epoch 252 out of 350###\n",
      "\t---Training Loss: 1.644e+01,\n",
      "\t---Validation Loss: 1.818e+01\n",
      "Validation Loss: 1.813e+01\n",
      "####Epoch 253 out of 350###\n",
      "\t---Training Loss: 1.635e+01,\n",
      "\t---Validation Loss: 1.813e+01\n",
      "Validation Loss: 1.807e+01\n",
      "####Epoch 254 out of 350###\n",
      "\t---Training Loss: 1.626e+01,\n",
      "\t---Validation Loss: 1.807e+01\n",
      "Validation Loss: 1.802e+01\n",
      "####Epoch 255 out of 350###\n",
      "\t---Training Loss: 1.618e+01,\n",
      "\t---Validation Loss: 1.802e+01\n",
      "Validation Loss: 1.797e+01\n",
      "####Epoch 256 out of 350###\n",
      "\t---Training Loss: 1.610e+01,\n",
      "\t---Validation Loss: 1.797e+01\n",
      "Validation Loss: 1.792e+01\n",
      "####Epoch 257 out of 350###\n",
      "\t---Training Loss: 1.602e+01,\n",
      "\t---Validation Loss: 1.792e+01\n",
      "Validation Loss: 1.788e+01\n",
      "####Epoch 258 out of 350###\n",
      "\t---Training Loss: 1.595e+01,\n",
      "\t---Validation Loss: 1.788e+01\n",
      "Validation Loss: 1.784e+01\n",
      "####Epoch 259 out of 350###\n",
      "\t---Training Loss: 1.587e+01,\n",
      "\t---Validation Loss: 1.784e+01\n",
      "Validation Loss: 1.780e+01\n",
      "####Epoch 260 out of 350###\n",
      "\t---Training Loss: 1.580e+01,\n",
      "\t---Validation Loss: 1.780e+01\n",
      "Validation Loss: 1.776e+01\n",
      "####Epoch 261 out of 350###\n",
      "\t---Training Loss: 1.574e+01,\n",
      "\t---Validation Loss: 1.776e+01\n",
      "Validation Loss: 1.773e+01\n",
      "####Epoch 262 out of 350###\n",
      "\t---Training Loss: 1.567e+01,\n",
      "\t---Validation Loss: 1.773e+01\n",
      "Validation Loss: 1.769e+01\n",
      "####Epoch 263 out of 350###\n",
      "\t---Training Loss: 1.561e+01,\n",
      "\t---Validation Loss: 1.769e+01\n",
      "Validation Loss: 1.766e+01\n",
      "####Epoch 264 out of 350###\n",
      "\t---Training Loss: 1.555e+01,\n",
      "\t---Validation Loss: 1.766e+01\n",
      "Validation Loss: 1.762e+01\n",
      "####Epoch 265 out of 350###\n",
      "\t---Training Loss: 1.549e+01,\n",
      "\t---Validation Loss: 1.762e+01\n",
      "Validation Loss: 1.759e+01\n",
      "####Epoch 266 out of 350###\n",
      "\t---Training Loss: 1.543e+01,\n",
      "\t---Validation Loss: 1.759e+01\n",
      "Validation Loss: 1.756e+01\n",
      "####Epoch 267 out of 350###\n",
      "\t---Training Loss: 1.537e+01,\n",
      "\t---Validation Loss: 1.756e+01\n",
      "Validation Loss: 1.753e+01\n",
      "####Epoch 268 out of 350###\n",
      "\t---Training Loss: 1.532e+01,\n",
      "\t---Validation Loss: 1.753e+01\n",
      "Validation Loss: 1.750e+01\n",
      "####Epoch 269 out of 350###\n",
      "\t---Training Loss: 1.527e+01,\n",
      "\t---Validation Loss: 1.750e+01\n",
      "Validation Loss: 1.747e+01\n",
      "####Epoch 270 out of 350###\n",
      "\t---Training Loss: 1.522e+01,\n",
      "\t---Validation Loss: 1.747e+01\n",
      "Validation Loss: 1.745e+01\n",
      "####Epoch 271 out of 350###\n",
      "\t---Training Loss: 1.517e+01,\n",
      "\t---Validation Loss: 1.745e+01\n",
      "Validation Loss: 1.742e+01\n",
      "####Epoch 272 out of 350###\n",
      "\t---Training Loss: 1.512e+01,\n",
      "\t---Validation Loss: 1.742e+01\n",
      "Validation Loss: 1.740e+01\n",
      "####Epoch 273 out of 350###\n",
      "\t---Training Loss: 1.508e+01,\n",
      "\t---Validation Loss: 1.740e+01\n",
      "Validation Loss: 1.738e+01\n",
      "####Epoch 274 out of 350###\n",
      "\t---Training Loss: 1.504e+01,\n",
      "\t---Validation Loss: 1.738e+01\n",
      "Validation Loss: 1.736e+01\n",
      "####Epoch 275 out of 350###\n",
      "\t---Training Loss: 1.499e+01,\n",
      "\t---Validation Loss: 1.736e+01\n",
      "Validation Loss: 1.734e+01\n",
      "####Epoch 276 out of 350###\n",
      "\t---Training Loss: 1.495e+01,\n",
      "\t---Validation Loss: 1.734e+01\n",
      "Validation Loss: 1.732e+01\n",
      "####Epoch 277 out of 350###\n",
      "\t---Training Loss: 1.492e+01,\n",
      "\t---Validation Loss: 1.732e+01\n",
      "Validation Loss: 1.730e+01\n",
      "####Epoch 278 out of 350###\n",
      "\t---Training Loss: 1.488e+01,\n",
      "\t---Validation Loss: 1.730e+01\n",
      "Validation Loss: 1.729e+01\n",
      "####Epoch 279 out of 350###\n",
      "\t---Training Loss: 1.484e+01,\n",
      "\t---Validation Loss: 1.729e+01\n",
      "Validation Loss: 1.728e+01\n",
      "####Epoch 280 out of 350###\n",
      "\t---Training Loss: 1.481e+01,\n",
      "\t---Validation Loss: 1.728e+01\n",
      "Validation Loss: 1.727e+01\n",
      "####Epoch 281 out of 350###\n",
      "\t---Training Loss: 1.478e+01,\n",
      "\t---Validation Loss: 1.727e+01\n",
      "Validation Loss: 1.726e+01\n",
      "####Epoch 282 out of 350###\n",
      "\t---Training Loss: 1.474e+01,\n",
      "\t---Validation Loss: 1.726e+01\n",
      "Validation Loss: 1.725e+01\n",
      "####Epoch 283 out of 350###\n",
      "\t---Training Loss: 1.471e+01,\n",
      "\t---Validation Loss: 1.725e+01\n",
      "Validation Loss: 1.724e+01\n",
      "####Epoch 284 out of 350###\n",
      "\t---Training Loss: 1.468e+01,\n",
      "\t---Validation Loss: 1.724e+01\n",
      "Validation Loss: 1.723e+01\n",
      "####Epoch 285 out of 350###\n",
      "\t---Training Loss: 1.466e+01,\n",
      "\t---Validation Loss: 1.723e+01\n",
      "Validation Loss: 1.723e+01\n",
      "####Epoch 286 out of 350###\n",
      "\t---Training Loss: 1.463e+01,\n",
      "\t---Validation Loss: 1.723e+01\n",
      "Validation Loss: 1.723e+01\n",
      "####Epoch 287 out of 350###\n",
      "\t---Training Loss: 1.460e+01,\n",
      "\t---Validation Loss: 1.723e+01\n",
      "Validation Loss: 1.722e+01\n",
      "####Epoch 288 out of 350###\n",
      "\t---Training Loss: 1.458e+01,\n",
      "\t---Validation Loss: 1.722e+01\n",
      "Validation Loss: 1.722e+01\n",
      "####Epoch 289 out of 350###\n",
      "\t---Training Loss: 1.456e+01,\n",
      "\t---Validation Loss: 1.722e+01\n",
      "Validation Loss: 1.722e+01\n",
      "####Epoch 290 out of 350###\n",
      "\t---Training Loss: 1.453e+01,\n",
      "\t---Validation Loss: 1.722e+01\n",
      "Validation Loss: 1.721e+01\n",
      "####Epoch 291 out of 350###\n",
      "\t---Training Loss: 1.451e+01,\n",
      "\t---Validation Loss: 1.721e+01\n",
      "Validation Loss: 1.721e+01\n",
      "####Epoch 292 out of 350###\n",
      "\t---Training Loss: 1.449e+01,\n",
      "\t---Validation Loss: 1.721e+01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.721e+01\n",
      "####Epoch 293 out of 350###\n",
      "\t---Training Loss: 1.447e+01,\n",
      "\t---Validation Loss: 1.721e+01\n",
      "Validation Loss: 1.721e+01\n",
      "####Epoch 294 out of 350###\n",
      "\t---Training Loss: 1.445e+01,\n",
      "\t---Validation Loss: 1.721e+01\n",
      "Validation Loss: 1.720e+01\n",
      "####Epoch 295 out of 350###\n",
      "\t---Training Loss: 1.443e+01,\n",
      "\t---Validation Loss: 1.720e+01\n",
      "Validation Loss: 1.720e+01\n",
      "####Epoch 296 out of 350###\n",
      "\t---Training Loss: 1.441e+01,\n",
      "\t---Validation Loss: 1.720e+01\n",
      "Validation Loss: 1.720e+01\n",
      "####Epoch 297 out of 350###\n",
      "\t---Training Loss: 1.440e+01,\n",
      "\t---Validation Loss: 1.720e+01\n",
      "Validation Loss: 1.720e+01\n",
      "####Epoch 298 out of 350###\n",
      "\t---Training Loss: 1.438e+01,\n",
      "\t---Validation Loss: 1.720e+01\n",
      "Validation Loss: 1.720e+01\n",
      "####Epoch 299 out of 350###\n",
      "\t---Training Loss: 1.437e+01,\n",
      "\t---Validation Loss: 1.720e+01\n",
      "Validation Loss: 1.720e+01\n",
      "####Epoch 300 out of 350###\n",
      "\t---Training Loss: 1.435e+01,\n",
      "\t---Validation Loss: 1.720e+01\n",
      "Validation Loss: 1.720e+01\n",
      "####Epoch 301 out of 350###\n",
      "\t---Training Loss: 1.434e+01,\n",
      "\t---Validation Loss: 1.720e+01\n",
      "Validation Loss: 1.720e+01\n",
      "####Epoch 302 out of 350###\n",
      "\t---Training Loss: 1.432e+01,\n",
      "\t---Validation Loss: 1.720e+01\n",
      "Validation Loss: 1.721e+01\n",
      "####Epoch 303 out of 350###\n",
      "\t---Training Loss: 1.431e+01,\n",
      "\t---Validation Loss: 1.721e+01\n",
      "Validation Loss: 1.721e+01\n",
      "####Epoch 304 out of 350###\n",
      "\t---Training Loss: 1.430e+01,\n",
      "\t---Validation Loss: 1.721e+01\n",
      "Validation Loss: 1.721e+01\n",
      "####Epoch 305 out of 350###\n",
      "\t---Training Loss: 1.429e+01,\n",
      "\t---Validation Loss: 1.721e+01\n",
      "Validation Loss: 1.722e+01\n",
      "####Epoch 306 out of 350###\n",
      "\t---Training Loss: 1.428e+01,\n",
      "\t---Validation Loss: 1.722e+01\n",
      "Validation Loss: 1.722e+01\n",
      "####Epoch 307 out of 350###\n",
      "\t---Training Loss: 1.427e+01,\n",
      "\t---Validation Loss: 1.722e+01\n",
      "Validation Loss: 1.722e+01\n",
      "####Epoch 308 out of 350###\n",
      "\t---Training Loss: 1.426e+01,\n",
      "\t---Validation Loss: 1.722e+01\n",
      "Validation Loss: 1.723e+01\n",
      "####Epoch 309 out of 350###\n",
      "\t---Training Loss: 1.425e+01,\n",
      "\t---Validation Loss: 1.723e+01\n",
      "Validation Loss: 1.723e+01\n",
      "####Epoch 310 out of 350###\n",
      "\t---Training Loss: 1.424e+01,\n",
      "\t---Validation Loss: 1.723e+01\n",
      "Validation Loss: 1.724e+01\n",
      "####Epoch 311 out of 350###\n",
      "\t---Training Loss: 1.423e+01,\n",
      "\t---Validation Loss: 1.724e+01\n",
      "Validation Loss: 1.724e+01\n",
      "####Epoch 312 out of 350###\n",
      "\t---Training Loss: 1.422e+01,\n",
      "\t---Validation Loss: 1.724e+01\n",
      "Validation Loss: 1.725e+01\n",
      "####Epoch 313 out of 350###\n",
      "\t---Training Loss: 1.421e+01,\n",
      "\t---Validation Loss: 1.725e+01\n",
      "Validation Loss: 1.725e+01\n",
      "####Epoch 314 out of 350###\n",
      "\t---Training Loss: 1.420e+01,\n",
      "\t---Validation Loss: 1.725e+01\n",
      "Validation Loss: 1.725e+01\n",
      "####Epoch 315 out of 350###\n",
      "\t---Training Loss: 1.420e+01,\n",
      "\t---Validation Loss: 1.725e+01\n",
      "Validation Loss: 1.726e+01\n",
      "####Epoch 316 out of 350###\n",
      "\t---Training Loss: 1.419e+01,\n",
      "\t---Validation Loss: 1.726e+01\n",
      "Validation Loss: 1.726e+01\n",
      "####Epoch 317 out of 350###\n",
      "\t---Training Loss: 1.418e+01,\n",
      "\t---Validation Loss: 1.726e+01\n",
      "Validation Loss: 1.726e+01\n",
      "####Epoch 318 out of 350###\n",
      "\t---Training Loss: 1.417e+01,\n",
      "\t---Validation Loss: 1.726e+01\n",
      "Validation Loss: 1.727e+01\n",
      "####Epoch 319 out of 350###\n",
      "\t---Training Loss: 1.417e+01,\n",
      "\t---Validation Loss: 1.727e+01\n",
      "Validation Loss: 1.727e+01\n",
      "####Epoch 320 out of 350###\n",
      "\t---Training Loss: 1.416e+01,\n",
      "\t---Validation Loss: 1.727e+01\n",
      "Validation Loss: 1.728e+01\n",
      "####Epoch 321 out of 350###\n",
      "\t---Training Loss: 1.415e+01,\n",
      "\t---Validation Loss: 1.728e+01\n",
      "Validation Loss: 1.728e+01\n",
      "####Epoch 322 out of 350###\n",
      "\t---Training Loss: 1.414e+01,\n",
      "\t---Validation Loss: 1.728e+01\n",
      "Validation Loss: 1.728e+01\n",
      "####Epoch 323 out of 350###\n",
      "\t---Training Loss: 1.414e+01,\n",
      "\t---Validation Loss: 1.728e+01\n",
      "Validation Loss: 1.728e+01\n",
      "####Epoch 324 out of 350###\n",
      "\t---Training Loss: 1.413e+01,\n",
      "\t---Validation Loss: 1.728e+01\n",
      "Validation Loss: 1.728e+01\n",
      "####Epoch 325 out of 350###\n",
      "\t---Training Loss: 1.412e+01,\n",
      "\t---Validation Loss: 1.728e+01\n",
      "Validation Loss: 1.728e+01\n",
      "####Epoch 326 out of 350###\n",
      "\t---Training Loss: 1.412e+01,\n",
      "\t---Validation Loss: 1.728e+01\n",
      "Validation Loss: 1.728e+01\n",
      "####Epoch 327 out of 350###\n",
      "\t---Training Loss: 1.411e+01,\n",
      "\t---Validation Loss: 1.728e+01\n",
      "Validation Loss: 1.728e+01\n",
      "####Epoch 328 out of 350###\n",
      "\t---Training Loss: 1.411e+01,\n",
      "\t---Validation Loss: 1.728e+01\n",
      "Validation Loss: 1.727e+01\n",
      "####Epoch 329 out of 350###\n",
      "\t---Training Loss: 1.410e+01,\n",
      "\t---Validation Loss: 1.727e+01\n",
      "Validation Loss: 1.726e+01\n",
      "####Epoch 330 out of 350###\n",
      "\t---Training Loss: 1.409e+01,\n",
      "\t---Validation Loss: 1.726e+01\n",
      "Validation Loss: 1.725e+01\n",
      "####Epoch 331 out of 350###\n",
      "\t---Training Loss: 1.409e+01,\n",
      "\t---Validation Loss: 1.725e+01\n",
      "Validation Loss: 1.724e+01\n",
      "####Epoch 332 out of 350###\n",
      "\t---Training Loss: 1.409e+01,\n",
      "\t---Validation Loss: 1.724e+01\n",
      "Validation Loss: 1.723e+01\n",
      "####Epoch 333 out of 350###\n",
      "\t---Training Loss: 1.408e+01,\n",
      "\t---Validation Loss: 1.723e+01\n",
      "Validation Loss: 1.721e+01\n",
      "####Epoch 334 out of 350###\n",
      "\t---Training Loss: 1.408e+01,\n",
      "\t---Validation Loss: 1.721e+01\n",
      "Validation Loss: 1.720e+01\n",
      "####Epoch 335 out of 350###\n",
      "\t---Training Loss: 1.408e+01,\n",
      "\t---Validation Loss: 1.720e+01\n",
      "Validation Loss: 1.719e+01\n",
      "####Epoch 336 out of 350###\n",
      "\t---Training Loss: 1.407e+01,\n",
      "\t---Validation Loss: 1.719e+01\n",
      "Validation Loss: 1.718e+01\n",
      "####Epoch 337 out of 350###\n",
      "\t---Training Loss: 1.407e+01,\n",
      "\t---Validation Loss: 1.718e+01\n",
      "Validation Loss: 1.718e+01\n",
      "####Epoch 338 out of 350###\n",
      "\t---Training Loss: 1.407e+01,\n",
      "\t---Validation Loss: 1.718e+01\n",
      "Validation Loss: 1.717e+01\n",
      "####Epoch 339 out of 350###\n",
      "\t---Training Loss: 1.406e+01,\n",
      "\t---Validation Loss: 1.717e+01\n",
      "Validation Loss: 1.717e+01\n",
      "####Epoch 340 out of 350###\n",
      "\t---Training Loss: 1.406e+01,\n",
      "\t---Validation Loss: 1.717e+01\n",
      "Validation Loss: 1.717e+01\n",
      "####Epoch 341 out of 350###\n",
      "\t---Training Loss: 1.406e+01,\n",
      "\t---Validation Loss: 1.717e+01\n",
      "Validation Loss: 1.717e+01\n",
      "####Epoch 342 out of 350###\n",
      "\t---Training Loss: 1.405e+01,\n",
      "\t---Validation Loss: 1.717e+01\n",
      "Validation Loss: 1.717e+01\n",
      "####Epoch 343 out of 350###\n",
      "\t---Training Loss: 1.405e+01,\n",
      "\t---Validation Loss: 1.717e+01\n",
      "Validation Loss: 1.717e+01\n",
      "####Epoch 344 out of 350###\n",
      "\t---Training Loss: 1.405e+01,\n",
      "\t---Validation Loss: 1.717e+01\n",
      "Validation Loss: 1.718e+01\n",
      "####Epoch 345 out of 350###\n",
      "\t---Training Loss: 1.405e+01,\n",
      "\t---Validation Loss: 1.718e+01\n",
      "Validation Loss: 1.718e+01\n",
      "####Epoch 346 out of 350###\n",
      "\t---Training Loss: 1.404e+01,\n",
      "\t---Validation Loss: 1.718e+01\n",
      "Validation Loss: 1.718e+01\n",
      "####Epoch 347 out of 350###\n",
      "\t---Training Loss: 1.404e+01,\n",
      "\t---Validation Loss: 1.718e+01\n",
      "Validation Loss: 1.719e+01\n",
      "####Epoch 348 out of 350###\n",
      "\t---Training Loss: 1.404e+01,\n",
      "\t---Validation Loss: 1.719e+01\n",
      "Validation Loss: 1.719e+01\n",
      "####Epoch 349 out of 350###\n",
      "\t---Training Loss: 1.403e+01,\n",
      "\t---Validation Loss: 1.719e+01\n",
      "Validation Loss: 1.720e+01\n",
      "####Epoch 350 out of 350###\n",
      "\t---Training Loss: 1.403e+01,\n",
      "\t---Validation Loss: 1.720e+01\n",
      "Validation Loss: 1.720e+01\n",
      "####Epoch 351 out of 350###\n",
      "\t---Training Loss: 1.403e+01,\n",
      "\t---Validation Loss: 1.720e+01\n"
     ]
    }
   ],
   "source": [
    "loss_trval_mod1_arr=trainValModel(0,350,model1,optimMod1,loss_MLF1,xmod1_train,yTrain,xmod1_test,yTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_trval_mod1_arr[:,1])\n",
    "plt.plot(loss_trval_mod1_arr[:,2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: Final position average, standard deviation and median as input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2=Sequential()\n",
    "model2.add(Dense(10,activation='relu',input_shape=(3,)))\n",
    "model2.add(Dense(20,activation='relu'))\n",
    "model2.add(Dense(10,activation='relu'))\n",
    "model2.add(Dense(1,activation='linear'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile model and show summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(optimizer='adam',loss='mean_squared_error',metrics=['accuracy'])\n",
    "\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmod2_train=np.float32(Xall_train[['AvgFinalPos','StdDevFinalPos','MedianFinalPos']].values)\n",
    "xmod2_test=np.float32(Xall_test[['AvgFinalPos','StdDevFinalPos','MedianFinalPos']].values)\n",
    "ymod_train=np.float32(y_train.values)\n",
    "ymod_test=np.float32(y_test.values)\n",
    "\n",
    "print(xmod2_train.shape)\n",
    "print(ymod_train.shape)\n",
    "print(xmod2_test.shape)\n",
    "print(ymod_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histo2=model2.fit(xmod2_train,ymod_train,epochs=17,validation_data=(xmod2_test,ymod_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(histo2.history['loss'])\n",
    "plt.plot(histo2.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.predict(xmod2_test[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ymod_test[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3: Mean and standard deviation of grid and final race position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3=Sequential()\n",
    "model3.add(Dense(10,activation='relu',input_shape=(4,)))\n",
    "model3.add(Dense(20,activation='relu'))\n",
    "model3.add(Dense(10,activation='relu'))\n",
    "model3.add(Dense(1,activation='linear'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile model and show summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.compile(optimizer='adam',loss='mean_squared_error',metrics=['accuracy'])\n",
    "\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xall_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmod3_train=np.float32(Xall_train[['AvgFinalPos','StdDevFinalPos','AvgStartPos','StdDevStartPos']].values)\n",
    "xmod3_test=np.float32(Xall_test[['AvgFinalPos','StdDevFinalPos','AvgStartPos','StdDevStartPos']].values)\n",
    "ymod_train=np.float32(y_train.values)\n",
    "ymod_test=np.float32(y_test.values)\n",
    "\n",
    "print(xmod3_train.shape)\n",
    "print(ymod_train.shape)\n",
    "print(xmod3_test.shape)\n",
    "print(ymod_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histo3=model3.fit(xmod3_train,ymod_train,epochs=20,validation_data=(xmod3_test,ymod_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(histo3.history['loss'])\n",
    "plt.plot(histo3.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.predict(xmod3_test[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ymod_test[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4: Median and standard deviation of grid and final race position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4=Sequential()\n",
    "model4.add(Dense(10,activation='relu',input_shape=(4,)))\n",
    "model4.add(Dense(20,activation='relu'))\n",
    "model4.add(Dense(10,activation='relu'))\n",
    "model4.add(Dense(1,activation='linear'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile model and show summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4.compile(optimizer='adam',loss='mean_squared_error',metrics=['accuracy'])\n",
    "\n",
    "model4.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xall_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmod4_train=np.float32(Xall_train[['MedianFinalPos','StdDevFinalPos','MedianStartPos','StdDevStartPos']].values)\n",
    "xmod4_test=np.float32(Xall_test[['MedianFinalPos','StdDevFinalPos','MedianStartPos','StdDevStartPos']].values)\n",
    "ymod_train=np.float32(y_train.values)\n",
    "ymod_test=np.float32(y_test.values)\n",
    "\n",
    "print(xmod4_train.shape)\n",
    "print(ymod_train.shape)\n",
    "print(xmod4_test.shape)\n",
    "print(ymod_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histo4=model4.fit(xmod4_train,ymod_train,epochs=20,validation_data=(xmod4_test,ymod_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(histo4.history['loss'])\n",
    "plt.plot(histo4.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4.predict(xmod4_test[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ymod_test[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 5: Mean and standard deviation of grid and final race position and number of entered races"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5=Sequential()\n",
    "model5.add(Dense(10,activation='relu',input_shape=(5,)))\n",
    "model5.add(Dense(20,activation='relu'))\n",
    "model5.add(Dense(10,activation='relu'))\n",
    "model5.add(Dense(1,activation='linear'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile model and show summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5.compile(optimizer='adam',loss='mean_squared_error',metrics=['accuracy'])\n",
    "\n",
    "model5.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xall_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmod5_train=np.float32(Xall_train[['AvgFinalPos','StdDevFinalPos','AvgStartPos','StdDevStartPos','NumRaces']].values)\n",
    "xmod5_test=np.float32(Xall_test[['AvgFinalPos','StdDevFinalPos','AvgStartPos','StdDevStartPos','NumRaces']].values)\n",
    "ymod_train=np.float32(y_train.values)\n",
    "ymod_test=np.float32(y_test.values)\n",
    "\n",
    "print(xmod5_train.shape)\n",
    "print(ymod_train.shape)\n",
    "print(xmod5_test.shape)\n",
    "print(ymod_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histo5=model5.fit(xmod5_train,ymod_train,epochs=20,validation_data=(xmod5_test,ymod_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(histo5.history['loss'])\n",
    "plt.plot(histo5.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5.predict(xmod5_test[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 6: Mean and standard deviation of final race position and number of entered races"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model6=Sequential()\n",
    "model6.add(Dense(10,activation='relu',input_shape=(3,)))\n",
    "model6.add(Dense(20,activation='relu'))\n",
    "model6.add(Dense(10,activation='relu'))\n",
    "model6.add(Dense(1,activation='linear'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile model and show summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model6.compile(optimizer='adam',loss='mean_squared_error',metrics=['accuracy'])\n",
    "\n",
    "model6.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmod6_train=np.float32(Xall_train[['AvgFinalPos','StdDevFinalPos','NumRaces']].values)\n",
    "xmod6_test=np.float32(Xall_test[['AvgFinalPos','StdDevFinalPos','NumRaces']].values)\n",
    "ymod_train=np.float32(y_train.values)\n",
    "ymod_test=np.float32(y_test.values)\n",
    "\n",
    "print(xmod6_train.shape)\n",
    "print(ymod_train.shape)\n",
    "print(xmod6_test.shape)\n",
    "print(ymod_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histo6=model6.fit(xmod6_train,ymod_train,epochs=20,validation_data=(xmod6_test,ymod_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(histo6.history['loss'])\n",
    "plt.plot(histo6.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model6.predict(xmod6_test[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 7: Shallow and wide - Mean and standard deviation of grid and final race position and number of entered races"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model7=Sequential()\n",
    "model7.add(Dense(200,activation='relu',input_shape=(5,)))\n",
    "model7.add(Dense(1,activation='linear'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile model and show summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model7.compile(optimizer='adam',loss='mean_squared_error',metrics=['accuracy'])\n",
    "\n",
    "model7.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xall_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmod7_train=np.float32(Xall_train[['AvgFinalPos','StdDevFinalPos','AvgStartPos','StdDevStartPos','NumRaces']].values)\n",
    "xmod7_test=np.float32(Xall_test[['AvgFinalPos','StdDevFinalPos','AvgStartPos','StdDevStartPos','NumRaces']].values)\n",
    "ymod_train=np.float32(y_train.values)\n",
    "ymod_test=np.float32(y_test.values)\n",
    "\n",
    "print(xmod7_train.shape)\n",
    "print(ymod_train.shape)\n",
    "print(xmod7_test.shape)\n",
    "print(ymod_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histo7=model7.fit(xmod7_train,ymod_train,epochs=20,validation_data=(xmod7_test,ymod_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(histo7.history['loss'])\n",
    "plt.plot(histo7.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model7.predict(xmod5_test[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ymod_test[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 8: Deep and narrow - Mean and standard deviation of grid and final race position and number of entered races"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model8=Sequential()\n",
    "model8.add(Dense(10,activation='relu',input_shape=(5,)))\n",
    "model8.add(Dense(10,activation='relu'))\n",
    "model8.add(Dense(10,activation='relu'))\n",
    "model8.add(Dense(10,activation='relu'))\n",
    "model8.add(Dense(10,activation='relu'))\n",
    "model8.add(Dense(10,activation='relu'))\n",
    "model8.add(Dense(10,activation='relu'))\n",
    "model8.add(Dense(10,activation='relu'))\n",
    "model8.add(Dense(10,activation='relu'))\n",
    "model8.add(Dense(10,activation='relu'))\n",
    "model8.add(Dense(1,activation='linear'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile model and show summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model8.compile(optimizer='adam',loss='mean_squared_error',metrics=['accuracy'])\n",
    "\n",
    "model8.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xall_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmod8_train=np.float32(Xall_train[['AvgFinalPos','StdDevFinalPos','AvgStartPos','StdDevStartPos','NumRaces']].values)\n",
    "xmod8_test=np.float32(Xall_test[['AvgFinalPos','StdDevFinalPos','AvgStartPos','StdDevStartPos','NumRaces']].values)\n",
    "ymod_train=np.float32(y_train.values)\n",
    "ymod_test=np.float32(y_test.values)\n",
    "\n",
    "print(xmod8_train.shape)\n",
    "print(ymod_train.shape)\n",
    "print(xmod8_test.shape)\n",
    "print(ymod_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histo8=model8.fit(xmod8_train,ymod_train,epochs=20,validation_data=(xmod8_test,ymod_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(histo8.history['loss'])\n",
    "plt.plot(histo8.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model8.predict(xmod8_test[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ymod_test[:3]"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
